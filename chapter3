* Predictive Model Building: Balancing Performance, Complexity, and Big Data*
-----------------------------------------------------------------------------

How is performance defined for different machine learning problems? This chapter presents relevant performance measures for these different problems. 
Achieving performance goals - 3 factors: complexity of problem, complexity of model, and richness of data. 

Basic problem - Function approximation: 
---------------------------------------
Approximate a function to predict one variable from other variables. 
 - Variables being predicted: label, target, outcome. 
 - Variables being used to make the predictions: predictors, regressors, features, attributes. 
Feature engineering: Determining what attributes must be used for predictions. 
In this book, we will come across algorithms that can assign numerical values to attributes and can thus help us do feature engineering faster. 

Working with Training Data: 
---------------------------
Attributes such as gender, marital status - categorical/ factor variables
Attributes - repr by numbers - numeric/ real-valued
Important to differentiate between these types of variables as some algos may not handle one type or the other. 
Linear methods - usually require only numeric attributes
PLM - converts categorical, if any, to numeric before applying algos. 

Targets - can be real --> regression problem 
Linear regression means: solving a regression problem with linear methods. 
Targets - two-valued --> binary classification 
        - multiple discrete values --> multi-class classification 

Assessing Performance of Predictive Models: 
-------------------------------------------
Regression - MSE/ MAE (since numeric)
Classification - Misclassification error - the fraction of the examples that pred() classifies incorrectly.

But, how do we get an error for a new data point, which the algorithm had not seen before? 

Factors Driving Algorithm Choices and Performance - Complexity & Data: 
----------------------------------------------------------------------
The only performance that counts is the performance of the algo when run against new examples. 
Complex problem - requires complex decision boundaries - ensemble methods 
The same complex problem - if reduced to a simple problem by reducing data points - PLM might probably perform better. 
Simple problem - with less data - PLM will perform better. 
Complex methods are thus called for, only when complex data/ problem is under consideration. 
Thus, a data set size may dictate that a simple model fits a complex problem better than a complex model. 

ML algorithms - generate families of models and not just 1 model. Ensemble models yield more complex models than linear models, but both of these methods generate multiple models of varying complexity. 

Fitting a more complicated model to a simple problem is not going to improve performance. A more complicated problem with more complicated decision boundaries gives a complicated model an opportunity to outperform a simple linear model.

It is tempting to draw the conclusion that complex problems require complex models and simple problems can be solved with simple models. We also need to take the size of the dataset into consideration. With less data, a complex problem is reduced to a simple problem and hence, a simple model (PLM) might give almost similar performance as the complex model (ensemble). 

Factors Driving Predicitve Algorithm Performance: 
-------------------------------------------------
Thus, with a large data set, we are more entitled to using more complex models. But with size of dataset, the shape of the data also matters. Adding a column to the data increases degree of freedom and makes the model more complex. More complex model requires more data and hence, needs an increase in rows. Thus, it is like maintaining the aspect ratio of an image. 

In biology (genomics) and NLP, the number of attributes >> number of data points. PLM will perform better here than ensemble methods. Dat not quite enough to train a complex model. In NLP, the vocabulary for a document - attributes. Attribute matrix, thus has usually > 1000 columns (even after preprocessing - removing oft repeated words like 'and', 'the', etc.). Also, if N-grams (N words together as a column) are considered, attribute matrix becomes very large. 

Choosing an algorithm: Linear or Non-linear? 
--------------------------------------------
Linear: no. of columns > no. of rows and model - simple. 
Non-linear: else; when many more rows than columns of data. 
Choosing non-linear model entails training a number of different models of differing complexity. A model that performs the best on out-of-sample data will be chosen. These models are all generated during the training process. 
We devise numeric performance measures for predictive models and how to estimate model performance. 

Measuring the performance of Predictive Models: 
-----------------------------------------------
Two areas: 
1. Different metrics for different problems: MAE/ MSE for regression; mis-classification for classification. ROC (receiver operating curves) and AUC (area under the curve). 
2. Techniques to predict out-of-sample error estimates. 

Performance measures for Different Types of Problems: 
-----------------------------------------------------
MSE and MAE are by default the performance measures for regression problems - they deal with real numbers. 
Coding done in error_measures.py 

Simulating Performance of Deployed Models: 
------------------------------------------
Train and test data splits are important. Training set must be representative of the entire dataset. And test set is not to be used for training at all. Test set size os usually 25% - 35% of data (no hard rule). Note: don't take too much data out of training set as it will deter performance. 
N-fold cross validation (instead of a fixed holdout set): N-1 training sets and 1 test set - cycled through the entire dataset. Each set - roughly equal sizes. 
- This method - useful to get error bounds on the error from the different error measures from different runs. 
- Also can train more on the data and can give better generalizations. 
- Takes more time, though. So, if your training takes a lot of time, cross-validation not to be used. Go with fixed holdout set (training only for one pass through the data).
While sampling data for preparing training and test sets: make sure labels are distributed evenly and that no bias is introduced in training and testing sets. 
Sometimes, in the case of rare events (fraud/ ad-clicks): random sampling can under-represent/ over-represent sepcific label. Use stratified sampling in this case. Stratified sampling divides the data into separate subsets that are separately sampled and then recombined. 

After model has been trained, and once you fix on the hyper paramters, it is good to train once again on the entire dataset (training + holdout). More the training data, always the better. 
Deployed model should thus be trained on more data before releasing it into the wild. 

