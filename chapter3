* Predictive Model Building: Balancing Performance, Complexity, and Big Data*
-----------------------------------------------------------------------------

How is performance defined for different machine learning problems? This chapter presents relevant performance measures for these different problems. 
Achieving performance goals - 3 factors: complexity of problem, complexity of model, and richness of data. 

Basic problem - Function approximation: 
---------------------------------------
Approximate a function to predict one variable from other variables. 
 - Variables being predicted: label, target, outcome. 
 - Variables being used to make the predictions: predictors, regressors, features, attributes. 
Feature engineering: Determining what attributes must be used for predictions. 
In this book, we will come across algorithms that can assign numerical values to attributes and can thus help us do feature engineering faster. 

Working with Training Data: 
---------------------------
Attributes such as gender, marital status - categorical/ factor variables
Attributes - repr by numbers - numeric/ real-valued
Important to differentiate between these types of variables as some algos may not handle one type or the other. 
Linear methods - usually require only numeric attributes
PLM - converts categorical, if any, to numeric before applying algos. 

Targets - can be real --> regression problem 
Linear regression means: solving a regression problem with linear methods. 
Targets - two-valued --> binary classification 
        - multiple discrete values --> multi-class classification 

Assessing Performance of Predictive Models: 
-------------------------------------------
Regression - MSE/ MAE (since numeric)
Classification - Misclassification error - the fraction of the examples that pred() classifies incorrectly.

But, how do we get an error for a new data point, which the algorithm had not seen before? 

Factors Driving Algorithm Choices and Performance - Complexity & Data: 
----------------------------------------------------------------------
The only performance that counts is the performance of the algo when run against new examples. 
Complex problem - requires complex decision boundaries - ensemble methods 
The same complex problem - if reduced to a simple problem by reducing data points - PLM might probably perform better. 
Simple problem - with less data - PLM will perform better. 
Complex methods are thus called for, only when complex data/ problem is under consideration. 
Thus, a data set size may dictate that a simple model fits a complex problem better than a complex model. 

ML algorithms - generate families of models and not just 1 model. Ensemble models yield more complex models than linear models, but both of these methods generate multiple models of varying complexity. 

Fitting a more complicated model to a simple problem is not going to improve performance. A more complicated problem with more complicated decision boundaries gives a complicated model an opportunity to outperform a simple linear model.

It is tempting to draw the conclusion that complex problems require complex models and simple problems can be solved with simple models. We also need to take the size of the dataset into consideration. With less data, a complex problem is reduced to a simple problem and hence, a simple model (PLM) might give almost similar performance as the complex model (ensemble). 

Factors Driving Predicitve Algorithm Performance: 
-------------------------------------------------
Thus, with a large data set, we are more entitled to using more complex models. But with size of dataset, the shape of the data also matters. Adding a column to the data increases degree of freedom and makes the model more complex. More complex model requires more data and hence, needs an increase in rows. Thus, it is like maintaining the aspect ratio of an image. 

In biology (genomics) and NLP, the number of attributes >> number of data points. PLM will perform better here than ensemble methods. Dat not quite enough to train a complex model. In NLP, the vocabulary for a document - attributes. Attribute matrix, thus has usually > 1000 columns (even after preprocessing - removing oft repeated words like 'and', 'the', etc.). Also, if N-grams (N words together as a column) are considered, attribute matrix becomes very large. 

Choosing an algorithm: Linear or Non-linear? 
--------------------------------------------
Linear: no. of columns > no. of rows and model - simple. 
Non-linear: else; when many more rows than columns of data. 
Choosing non-linear model entails training a number of different models of differing complexity. A model that performs the best on out-of-sample data will be chosen. These models are all generated during the training process. 
We devise numeric performance measures for predictive models and how to estimate model performance. 

Measuring the performance of Predictive Models: 
-----------------------------------------------
Two areas: 
1. Different metrics for different problems: MAE/ MSE for regression; mis-classification for classification. ROC (receiver operating curves) and AUC (area under the curve). 
2. Techniques to predict out-of-sample error estimates. 

Performance measures for Different Types of Problems: 
-----------------------------------------------------
MSE and MAE are by default the performance measures for regression problems - they deal with real numbers. 
Coding done in error_measures.py 

Simulating Performance of Deployed Models: 
------------------------------------------
Train and test data splits are important. Training set must be representative of the entire dataset. And test set is not to be used for training at all. Test set size os usually 25% - 35% of data (no hard rule). Note: don't take too much data out of training set as it will deter performance. 
N-fold cross validation (instead of a fixed holdout set): N-1 training sets and 1 test set - cycled through the entire dataset. Each set - roughly equal sizes. 
- This method - useful to get error bounds on the error from the different error measures from different runs. 
- Also can train more on the data and can give better generalizations. 
- Takes more time, though. So, if your training takes a lot of time, cross-validation not to be used. Go with fixed holdout set (training only for one pass through the data).
While sampling data for preparing training and test sets: make sure labels are distributed evenly and that no bias is introduced in training and testing sets. 
Sometimes, in the case of rare events (fraud/ ad-clicks): random sampling can under-represent/ over-represent sepcific label. Use stratified sampling in this case. Stratified sampling divides the data into separate subsets that are separately sampled and then recombined. 

After model has been trained, and once you fix on the hyper paramters, it is good to train once again on the entire dataset (training + holdout). More the training data, always the better. 
Deployed model should thus be trained on more data before releasing it into the wild. 

Achieving Harmony between Model & Data:
---------------------------------------
What's this about? 
OLS can sometimes overfit a problem. Overfitting happens when the errors on the training set are significantly different than errors on the test set. To overcome overfitting, PLMs are used in the case of linear regression models. 
Modern ML algos generate a number of models of varying complexity and then use out-of-sample performance to balance model complexity, problem complexity and data set richness to determine which model to deploy.
OLS can sometimes overfit. It is missing an important feature of ML algorithms: there's no means to throttle it back when it overfits. So, we shall talk about forward stepwise regression and ridge regression to combat overfitting. 

Choosing a Model to balance Problem complexity, Model complexity and Dataset size: 
----------------------------------------------------------------------------------
Y = beta(X) + beta0
If X is invertible (X has equal number of rows and columns and all columns are linearly independent), a coefficient vector beta will fit the labels exactly. Too good to be true: overfitting. 
Source of overfitting: having too many columns in X. So get rid of some columns: how many should you eliminate? and which amongst the columns are to be eliminated? 
Brute force method for this: best subset selection. And better method: forward stepwise regression 

Pseudo code: Best subset selection: 
-----------------------------------
Init: out_of_sample_error = [] 
Break X and Y into train and test sets 
for i in range(ncol of X): 
    for each subset of X haveing i+1 columns: 
        OLS fit on each subset 
    out_of_sample_error.append(least error among subsets containing i+1 columns)
Pick subset corresponding to least overall error

// Here, even after finding the minimum error fit on say, 1 column subsets, we again find all 2-column subsets of X. 10 attributes would lead to 2^10 subsets. Ideally, to save time and computations, we would like to use the 1-col subset we found (with least error) and append a column to that to reduce the number of sets in the 2-col subsets of X. This idea: forward stepwise regression 

Pseudo code: Forward stepwise regression: 
-----------------------------------------
Idea: Start with 1-col subsets and then, given the best single column, find the best second column to append instead of evaluating all possible 2-col subsets. 

Init: out_of_sample_error = []
Init: new_X = [] 
Break X and Y into training and test sets 
for number of columns in X:
    for column not in new_X:                # we don't want repetition 
        find OLS on (new_X+each col of X)
    out_of_sample_error.append(least error)
    new_X.append(run with least error)

Both these methods return a paramterized (number of attributes and which all attributes) family of models of varying complexities. Final model is selected based on performance on out-of-sample error. 
Code for testing forward stepwise regression on the wine dataset in fwdStepwiseWine.py

Control Overfitting by Penalizing Regression Coefficients - Ridge Regression: 
-----------------------------------------------------------------------------
First introduction to PLMs. Another method to modify OLS regression to control model complexity and to avoid overfitting. 
Best subset regression and forward stepwise regression throttle back ordinary regression by limiting the number of attributes used. That's equivalent to imposing the constraint that some of the attribute coefficients = 0. 
Another approach - coefficient penalized regression; make coefficients close to 0, but not 0. One version of coefficient penalized regression: Ridge Regression. 
The minimization equation of Ridge regression has a parameter: alpha --> When alpha is 0, it reduces to OLS. When alpha is large, betas become very small. 
Minimization equation of RR is different from OLS in one term only: alpha*beta*beta(transpose). 

Predicting wine taste with Ridge Regression - ridgeWine.py - (parameter: alpha and metric: RMSE)
RocksVMines was classified before using OLS, fixing a threshold, and deciding. Classification of the same using RR - classifierRidgeRocksVMines.py (parameter: alpha and metric: RMSE)

Summary: 
--------
- Visual demonstration of problem complexity and model complexity. 
- Metrics for regression (MSE, MAE, RMSE) and classification (roc_curve, confusion matrix, auc).
- Holdout and n-fold cross validation techniques. 
- ML algos produce parameterized family of models - one is selected for deployment on the basis of OOS performance. 


