** Penalized Linear Regression ** 
---------------------------------
OLS will not perform great in practical cases. Better approach is to use penalized regression models. Ch3 - 2 extensions to OLS - fwd stepwise regression & ridge regression. Both reduced the amount of data available to OLS and used OOS error measurements to determine how much data resulted in best performance. 
OLS has some inherent overfitting - PRMs tame this overfit. RR - eg of PRMs. RR regulates overfitting by penalizing sum of regression coefficients squared. 
How does penalty method determine the nature of solution and what type of information is available about the solution? 

Why PLRMs are so useful: 
------------------------
1. Extremely fast model training: Training time must be low; we might need to change our models based on changes in the problem statement; also feature engineering/ selection go hand-in-hand with training. Hence, we should not be spending too much time waiting for the model to train. 

2. Variable importance information: Along with training, the model must also tell us the importance of each of the attributes. This makes our understanding easier. Variable importance can be achieved through PLRMs and ensemble methods as well. Important to get a feel for which variables need to be looked at more carefully. Highly ranked attributes contribute more to model's prediction than lesser-ranked attributes. 

Fast training + variable importance info -- makes PLMs better for first iteration understanding of a dataset. 

3. Fast evaluation when deployed: Hard to beat a linear model for evaluation speed. 1 multiply and 1 add for each attribute since it is linear model. So, fast. 

Reliable performance: PLRMs will generate reasonable answers to problems of all shapes and sizes. With some coaxing, they can become powerful. Ensemble methods can sometimes be used with PLRMs to enhance prediction. 

4. When attribute mx are not that tall compared to their width (or when they are sparse): generates sparse solutions. Sparse solutions means some coeff = 0, so easy to see what all variables can be discarded. Easier to see model-prediction-driving attributes. 

5. Problem may require linear model: Drug testing and insurance payouts: these require linear models and hence, PLRMs are better here. 

When to use Ensemble methods: 
-----------------------------
Only when PLRMs do not give good performance, try ensemble methods. When problem is complex and data is huge to resolve problem's complexities. Ensemble methods usually yield more information about the attributes. They can give second-order information about what pairs of attributes go well together for model's prediction than these attributes contributing alone. This info with PLRMs can help squeeze more performance out of PLRMs. 

PLR: Regulating Linear Regression for Optimum Performance: 
----------------------------------------------------------
- Linear methods work with numerical data only. If you have categorical data, we should encode it into numerical attributes. OLS minimizes error between the X and Y by finding approppriate betas. Finding betas by hand - difficult usually. Thus, use minimization techniques. This can find betas such that the error (average squared error) is smallest (not zero, if 0: overfits). 
PLRM - add a penalty term (like in RR) composed of the betas to the minimization problem. For PLRM, the goal is to minimize the average squared error as well as the product of betas. Making all bets 0 can min the penalty term but will give high prediction error. OTOH, OLS ssolution minimizes the prediction error but may result in large coefficient penalty, depending on how large lambda is. 

Why does this make sense: In subset selection method, there are only some attributes that get selected. For others, betas=0. Here, in PLRM, betas != 0, but can come close to it. 
Limiting cases: if lambda=0: the problem is OLS and if lambda=+inf: penalty on bets is so severe that it forces them all to 0. Since beta0 is not included in the penalty, prediction becomes constant independent of the x's. 

Other penalties - Manhattan and ElasticNet: 
RR - uses metric of Euclidean geometry, sum of squared betas. (l2 norm) 
Lasso/ Manhattan - sum of absolute values of betas. (l1 norm) - here, many coefficients become 0' leads to sparse answer for large to moderate values of lambda.
ElasticNet - includes both lasso and ridge: adjustable blend of both; (alpha=1 - all lasso penalty and no ridge; usual procedure: pick alpha and iterate over lambda values). 

Why Lasso penalty leads to sparse coefficient vectors? : Remember the two figures, where the unpenalized (OLS) curve and curve from the penalties are drawn in a single plane. If lambda is high, error contribution from the penalty is more and thus coefficients would be close to 0. If lambda is small, betas are not zero. The point that minimizes the sum of these two curves: will always be the tangential point between curve 1 and curve 2. 

For RR, these curves will meet at a point not on either axis. But in Lasso, it so happens that for medium to large lambda, this will always be at a point on one of the axes. Thus, one beta becomes 0. Thus, this coefficient is not considered for model building. Thus, Lasso helps in feature selection. It gives a sense of order between the atributes under consideration. Thus, by varying lambda from large to small, we can list the attributes with their order of importance. 

Solving the Penalized Linear Regression Problem: 
-------------------------------------------------
PLRM basically gets you to solve an optimization problem. Two main minimization procedures: LARS and Glmnet. Fast algos to train and are available as Python packages. 
LARS - closely related to forward stepwise regression. 
Glmnet - very fast and very general. 

Least Angle Regression & Relationship to Forward stepwise regression: 
---------------------------------------------------------------------
fwdstepwise - init betas=0; and at each step, find residuals after using variables already chosen; variable set with least residual - new variable unreservedly added. 
LARS - init betas=0; and at each step, find which attribute has largest correlation with residuals. if corr > 0, increment variable's coefficient by small amount by a factor of correlation; else decrease by small amount. 

LARS solutions are usually similar to Lasso (with maybe minor differences). ElasticNet solvers can be understood by looking at LARS algorithm. See larsWine2.py for LARS implementation. 
